{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNKT = set(string.punctuation + \"«»№_—\")\n",
    "STOPWORDS = set(stopwords.words(\"russian\"))\n",
    "\n",
    "stemmer = RussianStemmer(\"stemmer\")\n",
    "\n",
    "    \n",
    "def text_to_sentences(text):\n",
    "    return [sentence for sentence in sent_tokenize(text)]\n",
    "\n",
    "\n",
    "def is_punkt(word):\n",
    "    return all(char in PUNKT for char in word) or word in STOPWORDS\n",
    "\n",
    "\n",
    "def sentence_to_tokens(sentence):\n",
    "    return [word for word in wordpunct_tokenize(sentence) if not is_punkt(word)]\n",
    " \n",
    "    \n",
    "def parse(text):\n",
    "    for sentence in text_to_sentences(text):\n",
    "        yield sentence_to_tokens(sentence)\n",
    "\n",
    "        \n",
    "def get_index(values, p):\n",
    "    return len(values[np.cumsum(values / sum(values)) < p])\n",
    "\n",
    "\n",
    "def is_allowed(ch):\n",
    "    return (ch >= 'а' and ch <= 'я') or (ch >= 'А' and ch <= 'Я') or ch == ' '\n",
    "\n",
    "\n",
    "def filter_text(text):\n",
    "    return ''.join(char for char in text if is_allowed(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = '''\n",
    "Барак Обама принимает в Белом доме своего французского коллегу Николя Саркози.\n",
    "О возможном включении благотворительного фонда в список \"иностранных агентов\" 7 мая написала газета «Ведомости».\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(filter(lambda it: len(it) > 0, raw_data.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_names = pd.read_csv('data/russian_names.csv', sep=';', usecols=['Name'])\n",
    "russian_names = set(russian_names.values.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_surnames = pd.read_csv('data/russian_surnames.csv', sep=';', usecols=['Surname'])\n",
    "russian_surnames = set(russian_surnames.values.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_person(token: str) -> bool:\n",
    "    return token in russian_names or token in russian_surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_org(token: str) -> bool:\n",
    "    return token == 'OOO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(sentence: str, token: str) -> [int]:\n",
    "    last = -1\n",
    "    \n",
    "    while True:\n",
    "        last = sentence.find(token, last + 1)\n",
    "        \n",
    "        if last == -1:\n",
    "             break\n",
    "        \n",
    "        yield last\n",
    "\n",
    "        \n",
    "def extract_person(sentence: str, token: str):\n",
    "    return map(lambda it: (it, len(token), 'PERSON'), extract(sentence, token))\n",
    "\n",
    "\n",
    "def extract_org(sentence: str, token: str):\n",
    "    return map(lambda it: (it, len(token), 'ORG'), extract(sentence, token))\n",
    "\n",
    "\n",
    "def tag_to_str(tag) -> str:\n",
    "    return f'{tag[0]} {tag[1]} {tag[2]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data:\n",
    "    tags = []\n",
    "    \n",
    "    for token in sentence_to_tokens(sentence):\n",
    "        if is_person(token):\n",
    "            tags += list(extract_person(sentence, token))\n",
    "        elif is_org(token):\n",
    "            tags += list(extract_org(sentence, token))\n",
    "            \n",
    "    result = ' '.join(list(map(tag_to_str, tags))) + ' EOL'\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
